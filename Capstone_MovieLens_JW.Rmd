---
title: "Capstone: MovieLens Recommendation System Exploration"
output: pdf_document
date: "December 2024"
---

# Introduction

This study examines the various predictive approaches to creating a movie recommendation system by leveraging the MovieLens dataset, developed by the GroupLens research team at the University of Minnesota is a widely used resource in the field of recommender system research (Harper and Konstan 2015). Recommendation systems are critical tools designed to help users navigate vast collections of items, such as movies, products, or music, by predicting user preferences and delivering personalized suggestions. Platforms like Netflix, Amazon, Spotify, and LinkedIn rely on such systems to enhance user experience and drive engagement. The challenge of building effective recommendation systems lies in the dynamic nature of user preferences, which change over time, making the development of accurate, adaptable models challenging. Originating in 1997 as a successor to DEC's EachMovie recommender system, MovieLens offers a robust repository of user ratings and timestamps, forming user-item-rating-timestamp tuples that can be useful in personalization research. 

By examining traditional methods like basic prediction while accounting for various biases in the data, matrix factorization, and the Extreme Gradient Boosting (XGBOOST) machine learning technique, I aim to identify the most effective algorithms for predicting user ratings. The dataset contains >25 million ratings but here, I will use a version of the dataset that contains 10 million. I evaluate the multiple models based on their predictive accuracy, measured here by their residual mean squared error (RMSE). I also depict the data spatially in map form (**Appendix** below).

This study draws inspiration from a Netflix competition in 2006 that awarded $1 million for improving their recommendation algorithm by 10% (Stone 2009), and applies some of the techniques used by the winning team to the publicly available MovieLens data. Unlike Netflix's proprietary dataset, the MovieLens data allows open experimentation.  

 

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org/"))

# *Disclaimer: when this outputs in HTML and Word it is all in order, in pdf form it doesn't always output in the same order despite efforts to clear caches, child files, etc., so visualizing the data in plots sometimes comes after the predictive modeling even though that doesn't create a logical flow*

# Install the required packages and dataset
Packages <- c("pander", "vctrs", "readr", "caTools", "magrittr", "ggpubr","ggplot2","corrplot", "party", "MuMIn", "lme4", "drat", "xgboost", "caret", "dplyr", "plyr", "stringr", "gsubfn", "lubridate", "xts", "fpp2", "data.table", "sf", "spData", "tmap", "leaflet", "fuzzyjoin", "mapdeck", "data.table", "summarytools", "rsample", "xgboost", "recosystem", "stringr")

for (pkg in Packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  } else {
    library(pkg, character.only = TRUE)  }
}

Working_Dir <- "C:/Users/"
setwd(Working_Dir)

#Download file.
URL <- "https://files.grouplens.org/datasets/movielens/ml-10m.zip"
FiletoDownload <- "ml-10m.zip"
download.file(URL, FiletoDownload, mode="wb")

#Unzip the file if it was downloaded properly. Print outcome.
unzip(FiletoDownload, exdir = Working_Dir)

#Delete zipped folder
unlink(FiletoDownload)

#Configure Movies_Ratings object
Movie_Ratings <- as.data.frame(str_split(read_lines("./ml-10M100K/ratings.dat"), fixed("::"), simplify = TRUE), stringsAsFactors = FALSE)

colnames(Movie_Ratings) <- c("userId", "movieId", "rating", "timestamp")
Movie_Ratings <- Movie_Ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

#Configure Movies_Data object
Movies_Data <- as.data.frame(str_split(read_lines("./ml-10M100K/movies.dat"), fixed("::"), simplify = TRUE), stringsAsFactors = FALSE)

colnames(Movies_Data) <- c("movieId", "title", "genres")

Movies_Data <- Movies_Data %>% 
  mutate(movieId = as.integer(movieId), 
         title = as.character(title), 
         genres = as.character(genres))


#Combine movies with ratings
MovieLens_Dataset <- left_join(Movie_Ratings, Movies_Data, by = "movieId")

#Convert time into something discernible
MovieLens_Dataset$timestamp <- as.POSIXct(MovieLens_Dataset$timestamp, origin="1970-01-01")
MovieLens_Dataset$Year <- format(MovieLens_Dataset$timestamp, "%Y")
MovieLens_Dataset$Month <- format(MovieLens_Dataset$timestamp, "%B")
MovieLens_Dataset <- MovieLens_Dataset %>% mutate(Age = 2024 - as.numeric(Year))
MovieLens_Dataset$Age <- as.integer(MovieLens_Dataset$Age)
```



## Analysis

### Visually Explore Dataset

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "The distribution of movie ratings in the dataset."}
#Visualize the distribution of movie ratings.
Figure.1 <- ggplot(MovieLens_Dataset, aes(x = rating)) +
  geom_histogram(binwidth = 0.5, fill = "grey", color = "blue") +
  labs(title = "Histogram Distribution of Movie Ratings", x = "Movie Rating", y = "Frequency", caption = "Data Source: MovieLens") +
  theme_light()
Figure.1
```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "No clear pattern in rating frequency/year."}
#Examine the frequency of movies per year.
MovieLens_Dataset$rating <-as.numeric(MovieLens_Dataset$rating)
Figure.2 <- ggplot(MovieLens_Dataset, aes(x = as.numeric(Year))) +
  geom_histogram(binwidth = 0.5, fill = "grey", color = "blue") +
  labs(title = "Distribution of Movie Ratings per Year", x = "Rating", y = "Frequency", caption = "Data Source: MovieLens") +
  theme_light()
Figure.2
```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "Average movie rating (score, not frequency) decreases slightly over time but inconsequentially."}
#Visualize the average movie numerical rating per year.
Ave_Annual_Rating <- aggregate(rating ~ Year, data = MovieLens_Dataset, FUN = mean)
Figure.3 <- ggplot(Ave_Annual_Rating, aes(x = as.numeric(Year), y = rating)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "Average Movie Ratings by Year", x = "Year", y = "Average Movie Rating (0.0 - 5.0)", caption = "Data Source: MovieLens") +
  expand_limits(y = c(3, 4)) +
  theme_light()
Figure.3
```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "Slight, but relatively unimpactful, pattern in average movie ratings issued by month. Ratings are slightly higher during winter than summer which could be for various sociocultural reasons beyond this study."}
#Sample 50k rows for ease of graphing. Older movies have more time to be rated.
SubSample_50k <- MovieLens_Dataset[sample(nrow(MovieLens_Dataset), size = 50000), ]
# Average Movie Ratings By Month - Any monthly biases?
month_order <- c("January", "February", "March", "April", "May", "June", 
                 "July", "August", "September", "October", "November", "December")
Ave_Monthly_Rating <- aggregate(rating ~ Month, data = MovieLens_Dataset, FUN = mean)
Ave_Monthly_Rating$Month <- factor(Ave_Monthly_Rating$Month, levels = month_order)
Figure.4 <- ggplot(Ave_Monthly_Rating, aes(x = Month, y = rating, group = 1)) +
  geom_line(color = "blue", size = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Adjust angle and horizontal justification
  expand_limits(y = c(3, 4)) +
  ggtitle("Average Movie Ratings by Month", subtitle = "Spanning 1995 to 2009") +
  xlab("Month") +
  ylab("Average Movie Rating (0.0 - 5.0)") +
  labs(caption = "Data Source: MovieLens") +
  theme_light()
Figure.4

```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "Highly skewed distribution of ratings per user which will likely add bias to predictive analyses."}
#Examine the frequency of ratings per user.
SubSample_50k <- MovieLens_Dataset[sample(nrow(MovieLens_Dataset), size = 50000), ]
User_and_Ratings <- aggregate(rating ~ userId, data = SubSample_50k, FUN = length)
colnames(User_and_Ratings)[2] <- "Number_Ratings"
Figure.5 <- ggplot(User_and_Ratings, aes(x = Number_Ratings)) +
  geom_histogram(binwidth = 1, fill = "light blue", color = "black") +  ggtitle("Distribution of the Number of Ratings per User", subtitle = "Subsample of 50,000 records")+
  xlab("Frequency of Ratings")+
ylab("Frequency of User") +
  theme_minimal()
Figure.5

```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "A pattern between movie age and rating received emerges but the data does not yield cultural/film-based evidence of why this may occur."}
#Examine the relationship between movie age and rating received.
Figure.6 <- ggplot(SubSample_50k, aes(x = Age, y = rating)) +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), color = "blue", se = TRUE) + 
  labs(title = "Relationship Between Movie Age and Rating Received",
       x = "Movie Age", y = "Rating", caption = "Data Source: MovieLens") +
  theme_minimal()
Figure.6
```

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE, fig.cap = "Similar to Figure 5, it is likely that distribution of ratings per genre will influence upcoming predictive analyses."}
#Examine frequencies of genres
GenreCount.Dataframe <- MovieLens_Dataset %>% group_by(genres) %>% tally() %>% mutate(proportion = n/sum(n)) %>% top_n(25)  %>% filter(proportion > 0.01) %>% arrange(proportion)
invisible(as.data.frame(GenreCount.Dataframe))

Figure.7 <- GenreCount.Dataframe %>% ggplot(aes(x =reorder(genres,proportion, sum), y = proportion, fill = proportion)) + geom_bar(stat = "identity") + scale_fill_gradient(low = "blue", high = "red") +coord_flip() + labs(x="Genre", y="Proportion of total movies", title="Proportion of movie genres \n comprising the MovieLens Dataset (top 15)" , caption = "Data Source: MovieLens")
Figure.7
```



I examined various approaches to predicting the movie score as measured by RMSE. I began with predicting movie rating simply using the mean and then a vector near the mean (3.4). I then created four subsequent models that account for varying biases such as those introduced by the movie itself, user, genre, or movie's age.

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE}
# Final hold-out test dataset will be 10% of MovieLens data
set.seed(1) 
test_index <- createDataPartition(y = MovieLens_Dataset$rating, times = 1, p = 0.1, list = FALSE)
edx <- MovieLens_Dataset[-test_index,]
temp <- MovieLens_Dataset[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- suppressMessages(anti_join(temp, final_holdout_test))
edx <- rbind(edx, removed)

#Create Training and Testing Dataset to investigate various types of preliminary models.
set.seed(123)  
MovieLens_split <- group_initial_split(edx, group = userId, prop = 0.8)
training_dataset <- training(MovieLens_split)
testing_dataset <- testing(MovieLens_split)
rm(Movie_Ratings, Movies_Data, test_index, MovieLens_split)

#Ensure the same movies are in both the training and testing datasets. 
testing_dataset <- testing_dataset |> 
  semi_join(training_dataset, by = "movieId")
training_dataset <- testing_dataset |> 
  semi_join(testing_dataset, by = "movieId")

#Create basic RMSE function for upcoming testing results.
RMSE <- function(true_ratings, predicted_ratings) {
  rmse_value <- sqrt(mean((true_ratings - predicted_ratings)^2))
  return(rmse_value)
}
#Basic model that predicts movie rating with no influence from user and simply relies on the data's mean. 
mu_train <- mean(training_dataset$rating)
training_dataset <- training_dataset %>% mutate(mu = mu_train)
RMSE_Naive <- RMSE(testing_dataset$rating, mu_train)

# We can tell that the mean model is better than a model including any other number, which isn't surprising. If we create a vector where each value is 3.4 (very similar to the mean), and then examine the RMSE, we can see it gets worse. 
Prediction_3.4 <- rep(3.4, nrow(testing_dataset))
RMSE_3.4 <- RMSE(testing_dataset$rating, Prediction_3.4)

#Some films have different rating distributions than others simply because of a variety of biases at play. So, lets include a model term to include the average rating of film i. 
Movie_Bias <- training_dataset %>% dplyr::group_by(movieId) %>%
  dplyr::summarise(Movie_Bias = mean(rating - mu_train))


Prediction_MovBi <- testing_dataset %>% left_join(Movie_Bias, by = "movieId") %>%  mutate(prediction = mu_train + Movie_Bias) %>%  .$prediction
RMSE_Movie_Bias <- RMSE(testing_dataset$rating, Prediction_MovBi)

#Similar to bias in the films themselves, users may have be more likely to rate a movie higher or lower more frequently than other viewers.
# Ensure 'userId' is a character 

User_Bias <- training_dataset %>% left_join(Movie_Bias, by = "movieId") %>% dplyr::group_by(userId) %>%
  dplyr::summarise(User_Bias = mean(rating - mu_train - Movie_Bias, na.rm = TRUE))

Prediction_UserBi <- testing_dataset %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = 'userId') %>%
  mutate(prediction2 = mu_train + Movie_Bias + User_Bias) %>%
  .$prediction2

RMSE_User_Bias <- RMSE(testing_dataset$rating, Prediction_UserBi)

#Similar to biases above, genres vary from mu, which could further inform a prediction.

Genre_Bias <- training_dataset %>% left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = 'userId') %>%
  dplyr::group_by(genres) %>%
  dplyr::summarise(Genre_Bias = mean(rating - mu_train - Movie_Bias - User_Bias, na.rm = TRUE))

 
Prediction_GenreBi <- testing_dataset %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = 'userId') %>%
  left_join(Genre_Bias, by = 'genres') %>%
  mutate(prediction3 = mu_train + Movie_Bias + User_Bias + Genre_Bias) %>%
  .$prediction3

RMSE_Genre_Bias <- RMSE(testing_dataset$rating, Prediction_GenreBi)


#Similar to biases above, movie age adds bias because some movies have had more time to be viewed and rated by users. 

MovieAge_Bias <- training_dataset %>% 
  left_join(Movie_Bias, by = "movieId") %>%
    left_join(User_Bias, by = "userId") %>%
  left_join(Genre_Bias, by = 'genres') %>%
  dplyr::group_by(Age) %>%
  dplyr::summarise(Age_Bias = mean(rating - mu_train - Movie_Bias - User_Bias - Genre_Bias, na.rm = TRUE))
 
Prediction_AgeBi <- testing_dataset %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = 'userId') %>%
  left_join(Genre_Bias, by = 'genres') %>%
  left_join(MovieAge_Bias, by = 'Age') %>%
  mutate(prediction4 = mu_train + Movie_Bias + User_Bias + Genre_Bias + Age_Bias) %>% .$prediction4

RMSE_Age_Bias <- RMSE(testing_dataset$rating, Prediction_AgeBi)

RMSE_Table <- tibble(Model = c("Mean Model", "Single Value (3.4) Model", "Movie Bias Model", "Movie+User Bias Model", "Movie+User+Genre Bias Model", "Movie+User+Genre+Age Bias Model"), RMSE = format(c(RMSE_Naive, RMSE_3.4, RMSE_Movie_Bias, RMSE_User_Bias, RMSE_Genre_Bias, RMSE_Age_Bias), nsmall = 4))
pander(RMSE_Table)
```




It can be seen that with the last couple of models we begin to have indications of overfitting. Overfitting is when incorporating too many variables in a model results in a good fit of the training data as it often includes underlying patterns and noise and random fluctuations but poorer performance on new, unseen validation/test data.

I then attempted to implement Matrix Factorization, which is a dimensionality reduction technique that 1) still captures the latent relationships between users and movies, 2) does well to handle the sparsity of movie rating data caused by many users only rating a few movies, and 4) is well-suited to examine large datasets. See Chapter 33 of the text (Irizzary, R.A.)

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE}
#Best/simplest to recreate the test/training datasets here after above manipulation.
set.seed(123)  
MovieLens_split <- group_initial_split(edx, group = userId, prop = 0.8)
training_dataset <- training(MovieLens_split)
testing_dataset <- testing(MovieLens_split)
rm(Movie_Ratings, Movies_Data, test_index, MovieLens_split)

#Ensure the same movies are in both the training and testing datasets. 
testing_dataset <- testing_dataset |> 
  semi_join(training_dataset, by = "movieId")
training_dataset <- testing_dataset |> 
  semi_join(testing_dataset, by = "movieId")

training_dataset <- training_dataset %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = "userId") %>%
  left_join(Genre_Bias, by = "genres")


# 1. Adjust Ratings in the Training Dataset
training_dataset <- training_dataset %>%
  dplyr::mutate(adjusted_rating = rating - Movie_Bias - User_Bias - Genre_Bias)

# Convert the training data to the required format
train_data <- data_memory(
  user_index = training_dataset$userId,
  item_index = training_dataset$movieId,
  rating = training_dataset$adjusted_rating)

recommender <- Reco()
recommender$train(
  train_data,
  opts = list(
    dim = 10,          # Number of latent factors
    costp_l1 = 0.1,    # L1 regularization for user factors
    costq_l1 = 0.1,    # L1 regularization for item factors
    lrate = 0.1,       # Learning rate
    niter = 50,        # Number of iterations
    verbose = FALSE     # Don't pint training progress
  )
)

test_data <- data_memory(
  user_index = testing_dataset$userId,
  item_index = testing_dataset$movieId)

# 6. Make Predictions with Adjustments
predicted_adjusted_ratings <- recommender$predict(test_data, out_memory())

testing_dataset <- testing_dataset %>%
  dplyr::mutate(predicted_adjusted_ratings = predicted_adjusted_ratings) %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = "userId") %>%
  left_join(Genre_Bias, by = "genres")

testing_dataset <- testing_dataset %>%
  dplyr::mutate(predicted_ratings = predicted_adjusted_ratings + 
           Movie_Bias + User_Bias + Genre_Bias)


# Actual ratings from the testing dataset
actual_ratings <- testing_dataset$rating
predicted_ratings <- testing_dataset$predicted_ratings

RMSE_Factorization <- RMSE(actual_ratings, predicted_ratings)

RMSE_Table <- tibble(Model = c("Mean Model", "Single Value (3.4) Model", "Movie Bias Model", "Movie+User Bias Model", "Movie+User+Genre Bias Model", "Movie+User+Genre+Age Bias Model", "Factorization Model"), RMSE = format(c(RMSE_Naive, RMSE_3.4, RMSE_Movie_Bias, RMSE_User_Bias, RMSE_Genre_Bias, RMSE_Age_Bias, RMSE_Factorization), nsmall = 4))
pander(RMSE_Table)
```



Next, I ran the matrix factorization on the final_holdout_test dataset. 


```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE}
test_data_holdout <- data_memory(
  user_index = final_holdout_test$userId,
  item_index = final_holdout_test$movieId)

# 6. Make Predictions with Adjustments
predicted_adjusted_ratings_holdout <- recommender$predict(test_data_holdout, out_memory())

final_holdout_test <- final_holdout_test %>%
  dplyr::mutate(predicted_adjusted_ratings_holdout = predicted_adjusted_ratings_holdout) %>%
  left_join(Movie_Bias, by = "movieId") %>%
  left_join(User_Bias, by = "userId") %>%
  left_join(Genre_Bias, by = "genres")

final_holdout_test <- final_holdout_test %>%
  dplyr::mutate(predicted_ratings = predicted_adjusted_ratings_holdout + 
                  Movie_Bias + User_Bias + Genre_Bias) %>%
  dplyr::filter(!is.na(predicted_ratings))

# Actual ratings from the testing dataset
actual_ratings <- final_holdout_test$rating
predicted_ratings <- final_holdout_test$predicted_ratings

RMSE_Factor_HoldOut <- RMSE(actual_ratings, predicted_ratings)

RMSE_Table <- tibble(Model = c("Mean Model", "Single Value (3.4) Model", "Movie Bias Model", "Movie+User Bias Model", "Movie+User+Genre Bias Model", "Factorization Model", "Factorization Model Holdout"), RMSE = c(RMSE_Naive, RMSE_3.4, RMSE_Movie_Bias, RMSE_User_Bias, RMSE_Genre_Bias, RMSE_Factorization, RMSE_Factor_HoldOut))
pander(RMSE_Table)

# Reorder RMSE values and plot
RMSE_Table <- RMSE_Table %>%
  dplyr::mutate(Model = factor(Model, levels = Model[order(-RMSE)]))

ggplot(RMSE_Table, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 0.865, linetype = "dashed", color = "red", size = 1) +
  geom_text(aes(label = round(RMSE, 4)), vjust = -0.5) +  # Add RMSE values above bars
  labs(title = "RMSE of Different Models", x = "Model", y = "RMSE") +
  coord_cartesian(ylim = c(0.6,1.15)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
```



Finally, I examined predicting movie rating with the XGBOOST technique. 
 
```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE}
#Convert from int to num for XGBoost
training_dataset <- training_dataset %>%
  dplyr::mutate(user_index = as.numeric(as.factor(userId)),
                movie_index = as.numeric(as.factor(movieId)))
final_holdout_test <- final_holdout_test %>%
  dplyr::mutate(user_index = as.numeric(as.factor(userId)),
    movie_index = as.numeric(as.factor(movieId)))

# Features: user_index and movie_index
train_matrix <- as.matrix(training_dataset %>% select(user_index, movie_index))
# Labels: adjusted_rating
train_labels <- training_dataset$adjusted_rating

# Define XGBoost parameters
params <- list(
  objective = "reg:squarederror",  # Regression with squared loss
  eta = 0.1,                       # Learning rate
  max_depth = 6,                   # Maximum depth of a tree
  subsample = 0.8,                 # Subsample ratio of the training instances
  colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)

# Train the model
xgb_model <- xgboost(
  data = train_matrix,
  label = train_labels,
  params = params,
  nrounds = 200,                   # Number of boosting rounds
  verbose = 0                      # Don't print training log
)

#Apply the same encoding to the testing data. Ensure that the factor levels match those in the training data.
test_matrix <- as.matrix(final_holdout_test %>% select(user_index, movie_index))

XGB_predicted_ratings <- predict(xgb_model, test_matrix)

final_holdout_test <- final_holdout_test %>%
  mutate(XGB_predicted_rating = XGB_predicted_ratings + User_Bias + Movie_Bias + Genre_Bias)

# Actual ratings
XGB_actual_ratings <- final_holdout_test$rating

# Predicted ratings
XGB_predicted_ratings <- final_holdout_test$XGB_predicted_rating
 
RMSE_XGB_HoldOut <- RMSE(XGB_actual_ratings, XGB_predicted_ratings)

RMSE_Table <- tibble(Model = c("Mean Model", "Single Value (3.4) Model", "Movie Bias Model", "Movie+User Bias Model", "Movie+User+Genre Bias Model", "Factorization Model", "Factorization Model Holdout", "XGBoost Model Holdout"), RMSE = format(c(RMSE_Naive, RMSE_3.4, RMSE_Movie_Bias, RMSE_User_Bias, RMSE_Genre_Bias, RMSE_Factorization, RMSE_Factor_HoldOut, RMSE_XGB_HoldOut), nsmall = 4))
pander(RMSE_Table)

# Reorder RMSE values and plot. 
RMSE_Table <- RMSE_Table %>%
  dplyr::mutate(RMSE = as.numeric(RMSE))
RMSE_Table <- RMSE_Table %>%
  dplyr::mutate(Model = factor(Model, levels = Model[order(-RMSE)]))

ggplot(RMSE_Table, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 0.865, linetype = "dashed", color = "red", size = 1) +
  geom_text(aes(label = round(RMSE, 4)), vjust = -0.5) +  # Add RMSE values above bars
  labs(title = "RMSE of Different Models", x = "Model", y = "RMSE") +
  coord_cartesian(ylim = c(0.6,1.15)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```



# Results

The final table depicts the RMSE values of various predictive models applied to the MovieLens dataset to evaluate their accuracy in predicting movie ratings. The "Mean Model" and "Single Value (3.4) Model" perform the worst, with RMSE values of 1.059 and 1.065, respectively, indicating limited predictive power due to their simplicity. The "Movie Bias Model" reduces RMSE significantly to 0.9389 by incorporating movie-specific effects. Adding user-specific effects in the "Movie+User Bias Model" lowers RMSE further to 0.8531. Including genre information in the "Movie+User+Genre Bias Model" reduces the RMSE to 0.8527, suggesting minimal additional improvement. The more advanced technique of the "Factorization Model" and its "Holdout" variant resulted in RMSE values of 0.8402 and 0.0597, respectively, demonstrating sound accuracy but minorly less accurate than the aforementioned bias model. The "XGBoost Model Holdout" also achieves a competitive RMSE of 0.8667 but still not as accurate as the bias model. 



# Conclusion
 
In this project, I evaluated various predictive models to determine their effectiveness in creating a movie recommendation system using the MovieLens dataset. The findings demonstrate that accounting for biases inherent in the data, such as those introduced by individual users, movies, and genres, enhances the predictive accuracy of the models. Simpler models, such as the "Mean Model" and the "Single Value (3.4) Model," performed poorly with RMSE values of 1.06 and 1.065, underscoring their inability to capture nuanced patterns in the data. However, incorporating movie-specific effects in the "Movie Bias Model" reduced the RMSE to 0.9389, marking a notable improvement. The addition of user-specific effects in the "Movie+User Bias Model" further enhanced accuracy, lowering the RMSE to 0.8531. This improvement highlights the importance of recognizing individual user tendencies in creating personalized recommendations.

Adding genre-specific information to the "Movie+User+Genre Bias Model" marginally reduced the RMSE to 0.8527, suggesting that genre information, while valuable, provides only slight additional explanatory power in this context. Advanced machine learning techniques, such as the "Factorization Model" and its "Holdout" variant, produced RMSE values of 0.8597 and 0.8667, respectively. While these models demonstrated sound predictive accuracy, they were slightly less effective than the "Movie+User+Genre Bias Model" in capturing the underlying patterns in the data. The "XGBoost Model Holdout," with an RMSE of 0.8667, exhibited comparable performance, reinforcing its reputation as a powerful algorithm for structured data.

These results align with findings from the aforementioneded 2006 Netflix competition (Stone 2009), which demonstrated that models leveraging bias corrections and matrix factorization approaches outperform simpler models. The improvements in accuracy achieved through incorporating user, movie, and genre effects reflect the complex and multifaceted nature of user preferences (also described in text (Irizarry, R.A)), emphasizing the value of personalized approaches in recommendation systems. However, the diminishing returns observed with increasingly complex models, such as XGBoost and matrix factorization, suggest a ceiling effect, where additional features or model complexity yield minimal improvements.

Ultimately, the "Movie+User+Genre Bias Model" emerged as the most effective predictive model for this dataset, offering the lowest RMSE value and a straightforward implementation, which highlights the importance of balancing complexity and interpretability when developing recommendation algorithms. When run on the final_holdout_test dataset, the factorization model performed better than the XGBoost model. While advanced methods like XGBoost and matrix factorization show promise, their marginal gains may not justify their increased computational demands in all scenarios. Future research could focus on incorporating temporal dynamics to account for changing user preferences or exploring hybrid approaches that combine the strengths of various models. These efforts have the potential to further improve predictive accuracy and enhance the user experience in recommendation systems.
 
 
 
# References

Irizarry, R.A. Introduction to Data Science, Data Analysis and Prediction Algorithms with R. https://rafalab.dfci.harvard.edu/dsbook/dataviz-distributions.html
Harper, F. M., & Konstan, J. A. (2015). The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4), 1-19.
Kuzelewska, U. (2014). Clustering algorithms in hybrid recommender system on movielens data. Studies in logic, grammar and rhetoric, 37(1), 125-139.
Stone, B. (2009, September 21). Netflix awards $1 million prize and starts a new contest. The New York Times. Retrieved from https://archive.nytimes.com/bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/


## Appendix

As a spatial analyst, I wanted to see which country names appeared in movie titles as well. Here, I join tables to map countries based on country names found in movie title.

```{r, message=TRUE, warning = FALSE, echo=FALSE, cache=FALSE}
#Read in country list
data(World, package = "tmap")
country_names <- World %>% pull(name)
country_names <- country_names[country_names != "Georgia"]
country_pattern <- paste(country_names, collapse = "|")

# Filter movie titles containing country names
Movie_Titles_Map <- tibble(title = unique(MovieLens_Dataset$title))
titles_with_countries <- Movie_Titles_Map %>%
  filter(str_detect(title, regex(country_pattern, ignore_case = TRUE)))

# Initialize a vector to store matching countries
matching_countries <- c()

# Iterate over each country name
for (country in country_names) {
  # Check if the country name is present in any of the filtered titles
  if (any(str_detect(titles_with_countries$title, regex(country, ignore_case = TRUE)))) {
    matching_countries <- c(matching_countries, country)
  }
}

# Remove duplicates
matching_countries <- unique(matching_countries)
world_map_data <- World %>%
  filter(name %in% matching_countries)

# Set tmap mode to plotting
suppressMessages(tmap_mode("plot"))

# Create the map with the title at the bottom center
tm_shape(World) +
  tm_polygons(col = "darkgrey", border.col = "white") +  # Basemap with other countries
  tm_shape(world_map_data) +
  tm_polygons(col = "darkgreen", border.col = "lightblue") +  # Highlighted countries
  tm_text("name", size = 0.5, col = "white", remove.overlap = TRUE) +  # Country names
  tm_layout(frame = FALSE) +
  tmap_options(bg.color = "black")
```